{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":982,"sourceType":"datasetVersion","datasetId":483}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Nandini `MDS202335`\n### Aritra `MCS202304`","metadata":{}},{"cell_type":"markdown","source":"# Importing necessary modules","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.distributions import Categorical\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport warnings\nfrom prettytable import PrettyTable\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-31T22:11:19.390267Z","iopub.execute_input":"2024-10-31T22:11:19.390648Z","iopub.status.idle":"2024-10-31T22:11:19.420765Z","shell.execute_reply.started":"2024-10-31T22:11:19.390611Z","shell.execute_reply":"2024-10-31T22:11:19.419871Z"},"trusted":true},"outputs":[],"execution_count":55},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:49:45.845368Z","iopub.execute_input":"2024-10-31T21:49:45.846126Z","iopub.status.idle":"2024-10-31T21:49:45.850944Z","shell.execute_reply.started":"2024-10-31T21:49:45.846084Z","shell.execute_reply":"2024-10-31T21:49:45.849972Z"},"trusted":true},"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/sms-spam-collection-dataset\"","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:49:46.760318Z","iopub.execute_input":"2024-10-31T21:49:46.760844Z","iopub.status.idle":"2024-10-31T21:49:46.765361Z","shell.execute_reply.started":"2024-10-31T21:49:46.760801Z","shell.execute_reply":"2024-10-31T21:49:46.764222Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Load and preprocess data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(os.path.join(DATA_PATH, \"spam.csv\"), encoding='latin1')\n\ndf = df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis = 1)\ndf = df.rename(columns={\"v1\": \"category\", \"v2\": \"message\"})\n\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:49:48.540409Z","iopub.execute_input":"2024-10-31T21:49:48.540811Z","iopub.status.idle":"2024-10-31T21:49:48.567725Z","shell.execute_reply.started":"2024-10-31T21:49:48.540767Z","shell.execute_reply":"2024-10-31T21:49:48.566801Z"},"trusted":true},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"  category                                            message\n0      ham  Go until jurong point, crazy.. Available only ...\n1      ham                      Ok lar... Joking wif u oni...\n2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n3      ham  U dun say so early hor... U c already then say...\n4      ham  Nah I don't think he goes to usf, he lives aro...\n5     spam  FreeMsg Hey there darling it's been 3 week's n...\n6      ham  Even my brother is not like to speak with me. ...\n7      ham  As per your request 'Melle Melle (Oru Minnamin...\n8     spam  WINNER!! As a valued network customer you have...\n9     spam  Had your mobile 11 months or more? U R entitle...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>message</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>spam</td>\n      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>spam</td>\n      <td>WINNER!! As a valued network customer you have...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>spam</td>\n      <td>Had your mobile 11 months or more? U R entitle...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"# An `END_TOKEN` for `LSTM` and `RNN` model to denote the `end``\n","metadata":{}},{"cell_type":"markdown","source":" \n`\"❙\"` *Vertical Bold bar* is used to denote the end\n","metadata":{}},{"cell_type":"code","source":"END_TOKEN = '\\u2759'\n\nprint(END_TOKEN)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:49:53.318773Z","iopub.execute_input":"2024-10-31T21:49:53.319156Z","iopub.status.idle":"2024-10-31T21:49:53.324616Z","shell.execute_reply.started":"2024-10-31T21:49:53.319119Z","shell.execute_reply":"2024-10-31T21:49:53.323520Z"},"trusted":true},"outputs":[{"name":"stdout","text":"❙\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Appending `END_TOKEN` to every messages","metadata":{}},{"cell_type":"code","source":"messages = list(df[\"message\"])\n\nfor idx in range(len(messages)):\n    messages[idx] += END_TOKEN","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:49:55.427128Z","iopub.execute_input":"2024-10-31T21:49:55.427585Z","iopub.status.idle":"2024-10-31T21:49:55.435906Z","shell.execute_reply.started":"2024-10-31T21:49:55.427544Z","shell.execute_reply":"2024-10-31T21:49:55.434910Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"messages[80]","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:49:56.179677Z","iopub.execute_input":"2024-10-31T21:49:56.180563Z","iopub.status.idle":"2024-10-31T21:49:56.186158Z","shell.execute_reply.started":"2024-10-31T21:49:56.180521Z","shell.execute_reply":"2024-10-31T21:49:56.185266Z"},"trusted":true},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"\"Sorry, I'll call later❙\""},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"# Remove meaningless characters to get rid of `hallucinations`","metadata":{}},{"cell_type":"code","source":"allowed_chars = [' ', '!', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '❙']","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:50:10.509658Z","iopub.execute_input":"2024-10-31T21:50:10.510511Z","iopub.status.idle":"2024-10-31T21:50:10.516772Z","shell.execute_reply.started":"2024-10-31T21:50:10.510474Z","shell.execute_reply":"2024-10-31T21:50:10.515805Z"},"trusted":true},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"# Build the corpus from the messages\n- All the messages are joined to get the corpus\n- Unallowed charcters are removed.\n- Corpus is truncated for better training and relevant contexts.","metadata":{}},{"cell_type":"code","source":"corpus = ' '.join(messages)\ncorpus = ''.join(char for char in corpus if char in allowed_chars)\ncorpus = corpus[:80000]\nchars = sorted(list(set(corpus))) \n\ndata_size, vocab_size = len(corpus), len(chars) \n\nprint(\"Corpus has {} characters, {} unique.\".format(data_size, vocab_size))","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:50:40.087191Z","iopub.execute_input":"2024-10-31T21:50:40.087848Z","iopub.status.idle":"2024-10-31T21:50:40.704836Z","shell.execute_reply.started":"2024-10-31T21:50:40.087800Z","shell.execute_reply":"2024-10-31T21:50:40.703798Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Corpus has 80000 characters, 74 unique.\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(chars)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:50:54.981188Z","iopub.execute_input":"2024-10-31T21:50:54.982070Z","iopub.status.idle":"2024-10-31T21:50:54.986629Z","shell.execute_reply.started":"2024-10-31T21:50:54.982026Z","shell.execute_reply":"2024-10-31T21:50:54.985685Z"},"trusted":true},"outputs":[{"name":"stdout","text":"[' ', '!', \"'\", '(', ')', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '❙']\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"# Define the `mapping` and the `inverse mapping` for vocab","metadata":{}},{"cell_type":"code","source":"char_to_ix = { ch:i for i,ch in enumerate(chars) }\nix_to_char = { i:ch for i,ch in enumerate(chars) }","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:50:59.505472Z","iopub.execute_input":"2024-10-31T21:50:59.506317Z","iopub.status.idle":"2024-10-31T21:50:59.510739Z","shell.execute_reply.started":"2024-10-31T21:50:59.506278Z","shell.execute_reply":"2024-10-31T21:50:59.509796Z"},"trusted":true},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"# Convert the stream of `characters` to sequence of `ints`\n","metadata":{}},{"cell_type":"code","source":"def get_sequence(input_string):\n    \"\"\"\n    Encodes a given string into a list of indices based on the char_to_ix mapping.\n\n    Parameters:\n    - input_string (str): The string to be encoded.\n    - char_to_ix (dict): A dictionary mapping characters to their respective indices.\n\n    Returns:\n    - List[int]: A list of indices corresponding to the characters in the input string.\n    \"\"\"\n    sequence = [char_to_ix[ch] for ch in input_string if ch in char_to_ix]\n    \n    return sequence","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:51:02.845801Z","iopub.execute_input":"2024-10-31T21:51:02.846174Z","iopub.status.idle":"2024-10-31T21:51:02.851666Z","shell.execute_reply.started":"2024-10-31T21:51:02.846141Z","shell.execute_reply":"2024-10-31T21:51:02.850505Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"corpus = get_sequence(corpus)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:51:07.915081Z","iopub.execute_input":"2024-10-31T21:51:07.915479Z","iopub.status.idle":"2024-10-31T21:51:07.922956Z","shell.execute_reply.started":"2024-10-31T21:51:07.915442Z","shell.execute_reply":"2024-10-31T21:51:07.922067Z"},"trusted":true},"outputs":[],"execution_count":31},{"cell_type":"code","source":"# Make the corpus array tensor\ndata = torch.tensor(corpus).to(device)\n\n# Add an extra dimension to dimension 1\ndata = torch.unsqueeze(data, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:51:10.931204Z","iopub.execute_input":"2024-10-31T21:51:10.931887Z","iopub.status.idle":"2024-10-31T21:51:10.947384Z","shell.execute_reply.started":"2024-10-31T21:51:10.931844Z","shell.execute_reply":"2024-10-31T21:51:10.946438Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":"## `myRNN` Architecture\n\nThe `myRNN` class implements a simple recurrent neural network with configurable layers, hidden size, and dropout:\n\n- **Parameters**:\n  - `input_size`: Number of input features in each time step of the sequence.\n  - `output_size`: Number of output features in each time step.\n  - `hidden_size`: Size of the hidden state vector.\n  - `num_layers`: Number of recurrent layers.\n  - `do_dropout`: Boolean indicating whether to apply dropout.\n\n- **Components**:\n  - **Dropout Layer**: If `do_dropout` is set to `True`, a dropout layer with a dropout rate of 0.5 is applied to the inputs, which helps prevent overfitting.\n  - **RNN Layer**: The core of the architecture, an `nn.RNN` layer, processes the input sequence using `num_layers` recurrent layers.\n  - **Decoder Layer**: An `nn.Linear` layer maps the final hidden state to the desired `output_size`, producing predictions for each time step.\n\n\n- **Forward Pass**:\n  - The input sequence is first converted to a one-hot encoding with shape `[sequence_length, batch_size, input_size]`.\n  - Dropout is applied to the input if enabled.\n  - The RNN processes the input, updating the hidden state at each step. The output of the RNN is passed to the decoder to generate predictions.\n  - The hidden state is detached to prevent gradient backpropagation through time steps across training batches.\n\n---\n\n## `myLSTM` Architecture\n\nThe `myLSTM` class implements a long short-term memory network, which is more complex than the standard RNN due to additional gating mechanisms:\n\n- **Parameters**:\n  - Similar to `myRNN`: `input_size`, `output_size`, `hidden_size`, `num_layers`, and `do_dropout`.\n\n\n- **Components**:\n  - **Dropout Layer**: Functions identical to the dropout in `myRNN`.\n  - **LSTM Layer**: The `nn.LSTM` layer includes gates that control information flow, helping mitigate issues like vanishing gradients in long sequences.\n  - **Decoder Layer**: As with `myRNN`, an `nn.Linear` layer maps the hidden states to the output size for prediction.\n\n\n- **Forward Pass**:\n  - The input sequence is converted to a one-hot encoding.\n  - Dropout is applied if enabled.\n  - The LSTM processes the input, updating both the hidden state and the cell state at each step. These states are passed to the decoder for predictions.\n  - Both the hidden state and cell state are detached to avoid backpropagation through the previous batches.","metadata":{}},{"cell_type":"code","source":"class myRNN(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size=512, num_layers=3, do_dropout=False):\n        super(myRNN, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.do_dropout = do_dropout\n        \n        self.dropout = nn.Dropout(0.5)\n        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n        self.decoder = nn.Linear(hidden_size, output_size)\n        \n        self.hidden_state = None \n    \n    def forward(self, input_seq):\n        x = nn.functional.one_hot(input_seq, self.input_size).float()\n        if self.do_dropout:\n            x = self.dropout(x)\n        x, new_hidden_state = self.rnn(x, self.hidden_state)\n        output = self.decoder(x)\n        self.hidden_state = new_hidden_state.detach() \n        return output\n    \n    def save_model(self, path):\n        torch.save(self.state_dict(), path)\n    \n    def load_model(self, path):\n        try:\n            self.load_state_dict(torch.load(path))\n        except Exception as err:\n            print(\"Error loading model from file\", path)\n            print(err)\n            print(\"Initializing model weights to default\")\n            self.__init__(self.input_size, self.output_size, self.hidden_size, self.num_layers)\n\nclass myLSTM(nn.Module):\n    def __init__(self, input_size, output_size, hidden_size=512, num_layers=3, do_dropout=False):\n        super(myLSTM, self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.do_dropout = do_dropout\n        \n        self.dropout = nn.Dropout(0.5)\n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers)\n        self.decoder = nn.Linear(hidden_size, output_size)\n        \n        self.internal_state = None \n    def forward(self, input_seq):\n        x = nn.functional.one_hot(input_seq, self.input_size).float()\n        if self.do_dropout:\n            x = self.dropout(x)\n        x, new_internal_state = self.lstm(x, self.internal_state)\n        output = self.decoder(x)\n        self.internal_state = (new_internal_state[0].detach(), new_internal_state[1].detach())\n        return output\n    \n    def save_model(self, path):\n        torch.save(self.state_dict(), path)\n    \n    def load_model(self, path):\n        try:\n            self.load_state_dict(torch.load(path))\n        except Exception as err:\n            print(\"Error loading model from file\", path)\n            print(err)\n            print(\"Initializing model weights to default\")\n            self.__init__(self.input_size, self.output_size, self.hidden_size, self.num_layers)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:49:00.045477Z","iopub.execute_input":"2024-10-31T21:49:00.046304Z","iopub.status.idle":"2024-10-31T21:49:00.063336Z","shell.execute_reply.started":"2024-10-31T21:49:00.046266Z","shell.execute_reply":"2024-10-31T21:49:00.062527Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Get Number of Parameters in a Model\n\n- This function calculates the total number of parameters in a given PyTorch model.\n\n- It is useful for comparing model complexity across different architectures.\n","metadata":{}},{"cell_type":"code","source":"def get_n_params(model):\n    \"\"\"\n    returns the number of parameters of a model\n    \"\"\"\n    np=0\n    for p in list(model.parameters()):\n        np += p.nelement()\n    return np","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:51:19.270883Z","iopub.execute_input":"2024-10-31T21:51:19.271271Z","iopub.status.idle":"2024-10-31T21:51:19.276203Z","shell.execute_reply.started":"2024-10-31T21:51:19.271219Z","shell.execute_reply":"2024-10-31T21:51:19.275285Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"### `train` function to train the LSTM and RNN models","metadata":{}},{"cell_type":"code","source":"def train(model, epoch, seq_len = 200):\n    \"\"\"\n    Train the model on a sequence data for one epoch.\n\n    This function trains the given RNN model for a single epoch using sequence data, \n    computes the average loss, and periodically displays a sample sequence generated by the model.\n\n    Parameters:\n    model : nn.Module\n        The RNN model to be trained.\n    epoch : int\n        The current epoch number, used to control logging and sample generation.\n    seq_len : int, optional\n        The length of the input sequence (default is 200).\n    \n    Returns:\n    float\n        The average training loss over all batches for the epoch.\n    \n    Process:\n    - Sets the model to training mode.\n    - Defines a cross-entropy loss function.\n    - Randomly selects a data pointer within the sequence range.\n    - For each batch in the sequence:\n      - Extracts the input and target sequences.\n      - Computes the model output and the loss.\n      - Backpropagates the loss and updates model parameters.\n    - Every 10 epochs or during early epochs (1-3), generates and displays a sample output.\n      - This sample is generated by feeding the model an initial input and sampling subsequent characters.\n    \"\"\"\n    model.train()\n    loss_fn = nn.CrossEntropyLoss()\n    \n    \n    test_output_len = seq_len\n    \n    data_ptr = np.random.randint(seq_len)\n    running_loss = 0\n    n = 0;\n    \n    if epoch % 10 == 0 or epoch == 1 or epoch == 2 or epoch == 3:\n        print(\"\\nStart of Epoch: {0}\".format(epoch))\n        \n    while True:\n        input_seq = data[data_ptr : data_ptr+seq_len]\n        target_seq = data[data_ptr+1 : data_ptr+seq_len+1]\n        input_seq.to(device)\n        target_seq.to(device)\n        \n        optimizer.zero_grad()\n        output = model(input_seq)\n        loss = loss_fn(torch.squeeze(output), torch.squeeze(target_seq))\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n\n        data_ptr += seq_len\n        if data_ptr + seq_len + 1 > data_size:\n            break\n        \n        n = n+1\n            \n    if epoch % 10 == 0 or epoch == 1 or epoch == 2 or epoch == 3:\n        model.eval()\n        data_ptr = 0\n\n        rand_index = np.random.randint(data_size-1)\n        input_seq = data[rand_index : rand_index+1]\n\n        \n        test_output = \"\"\n        while True:\n            output = model(input_seq)\n\n            output = F.softmax(torch.squeeze(output), dim=0)\n            dist = Categorical(output)\n            index = dist.sample().item()\n            \n\n            test_output += ix_to_char[index]\n\n            input_seq[0][0] = index\n            data_ptr += 1\n\n            if data_ptr > test_output_len:\n                break\n        print(\"----------\")\n        print(\"TRAIN Sample\")\n        print(test_output)\n        print(\"----------\")\n        print(\"End of Epoch: {0} \\t Loss: {1:.8f}\".format(epoch, running_loss / n))\n    \n    return running_loss / n","metadata":{"execution":{"iopub.status.busy":"2024-10-31T21:51:19.806047Z","iopub.execute_input":"2024-10-31T21:51:19.806421Z","iopub.status.idle":"2024-10-31T21:51:19.819532Z","shell.execute_reply.started":"2024-10-31T21:51:19.806385Z","shell.execute_reply":"2024-10-31T21:51:19.818477Z"},"trusted":true},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"### `test` function to evaluate the performance of message completion","metadata":{}},{"cell_type":"code","source":"def test(model, x, output_len=50, T=0): \n    \"\"\"\n    Generate a sequence of characters using the provided model based on an initial input string.\n\n    This function takes an initial input string, encodes it as indices, and uses the \n    specified RNN model to generate a sequence of characters of a given length. \n    The generation can be influenced by the temperature parameter (T) to control randomness.\n\n    Parameters:\n    model : nn.Module\n        The trained RNN model to be used for character generation.\n    x : str\n        The initial input string from which to start generating the output sequence.\n    output_len : int, optional\n        The maximum length of the generated output sequence (default is 50).\n    T : float, optional\n        The temperature parameter that influences the randomness of the predictions. \n        A value of 0 means that the model will always select the most probable character \n        (greedy approach). A value other than 1 will sample from the output probability distribution.\n\n    Returns:\n    None\n\n    Process:\n    - The model is set to evaluation mode.\n    - The input string is converted to a tensor of character indices.\n    - A subset of the input sequence is selected to be used for generation.\n    - The model generates characters one at a time until the specified output length \n      is reached or an end token is encountered.\n    - The generated characters are accumulated into a result string and printed along \n      with the initial input context.\n    \"\"\"\n    model.eval()\n    \n    data_ptr = 0\n    hidden_state = None    \n    \n    \n    input_seq = torch.tensor([char_to_ix[char] for char in x]).to(device)\n    input_seq = torch.unsqueeze(input_seq, dim=1)\n    y = 30\n    y = min(y, len(input_seq) // 2)\n    x = 0\n    input_seq = input_seq[x:x+y]\n    test_input = ''.join(ix_to_char[int(ix)] for ix in input_seq)\n\n    output = model(input_seq)\n    \n    input_seq = data[x+y:x+y+1]\n    \n    test_output = \"\"\n    \n    while True:\n        output = model(input_seq)\n        \n        output = F.softmax(torch.squeeze(output), dim=0)\n        dist = Categorical(output)\n        index = dist.sample().item()\n        \n        most_probable_index = torch.argmax(output).item()\n        \n        if T == 0:\n            index = most_probable_index\n        test_output += ix_to_char[index]\n        \n        if ix_to_char[index] == END_TOKEN:\n            break\n        \n        input_seq[0][0] = index\n        data_ptr += 1\n        \n        if data_ptr  > output_len:\n            break\n\n    print(f\"{model.__class__.__name__} completed:\")\n    print(f\"context: {test_input}\")\n    print(f\"output: {test_output}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-31T22:01:51.006478Z","iopub.execute_input":"2024-10-31T22:01:51.006898Z","iopub.status.idle":"2024-10-31T22:01:51.018404Z","shell.execute_reply.started":"2024-10-31T22:01:51.006858Z","shell.execute_reply":"2024-10-31T22:01:51.017496Z"},"trusted":true},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"# Training the LSTM model","metadata":{}},{"cell_type":"code","source":"hidden_size = 512\nnum_layers = 3       \nlr = 0.002          \n\nmodel_lstm = myLSTM(vocab_size, vocab_size, hidden_size, num_layers).to(device)\noptimizer = torch.optim.Adam(model_lstm.parameters(), lr=lr)\n\nbest_model_lstm = myLSTM(vocab_size, vocab_size, hidden_size, num_layers).to(device)\nbest_lstm_loss = 10000\n \nfor epoch in tqdm(range(1, 101)):\n    epoch_loss = train(model_lstm, epoch, 50)\n    if epoch_loss < best_lstm_loss:\n        best_lstm_loss = epoch_loss\n        best_model_lstm.load_state_dict(model_lstm.state_dict())","metadata":{"execution":{"iopub.status.busy":"2024-10-31T09:06:08.780666Z","iopub.execute_input":"2024-10-31T09:06:08.781035Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7e9b78b12774915a726d978680b16b1"}},"metadata":{}},{"name":"stdout","text":"\nStart of Epoch: 1\n----------\nTRAIN Sample\nlock that intr u tear of doing tho end to MANESS! I\n----------\nEnd of Epoch: 1 \t Loss: 0.28820632\n\nStart of Epoch: 2\n----------\nTRAIN Sample\nok, I beilge in the pls on my somect.❙ I lave m che\n----------\nEnd of Epoch: 2 \t Loss: 0.22217210\n\nStart of Epoch: 3\n----------\nTRAIN Sample\nng about the mistumf ot the u'll then i go soworrow\n----------\nEnd of Epoch: 3 \t Loss: 0.13288940\n\nStart of Epoch: 10\n----------\nTRAIN Sample\nre.❙ As per u bit of people that dont kear and wank\n----------\nEnd of Epoch: 10 \t Loss: 0.01264650\n\nStart of Epoch: 20\n----------\nTRAIN Sample\ner already?❙ Yup... Ok lar... Joking wif u oni...❙ \n----------\nEnd of Epoch: 20 \t Loss: 0.02349879\n\nStart of Epoch: 30\n----------\nTRAIN Sample\nthat be a win FA Cup final tkts 2 a wAly comp to wi\n----------\nEnd of Epoch: 30 \t Loss: 0.05197851\n\nStart of Epoch: 40\n----------\nTRAIN Sample\nrconter! Nit me know so nice is so itker ereding❙ Y\n----------\nEnd of Epoch: 40 \t Loss: 0.14801698\n\nStart of Epoch: 50\n----------\nTRAIN Sample\ne coans if thats the way its gota b❙ Even my sign, \n----------\nEnd of Epoch: 50 \t Loss: 0.06381190\n\nStart of Epoch: 60\n----------\nTRAIN Sample\ne or that that's leare that dixls❙ You like dire th\n----------\nEnd of Epoch: 60 \t Loss: 0.02203953\n\nStart of Epoch: 70\n----------\nTRAIN Sample\nhowey account has been refilled su nire with m... ❙\n----------\nEnd of Epoch: 70 \t Loss: 0.02347969\n\nStart of Epoch: 80\n----------\nTRAIN Sample\n.. As I entered my house now...❙ FreeMsg Why haven'\n----------\nEnd of Epoch: 80 \t Loss: 0.03963449\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"def complete_message_using_lstm(index, T=0):\n    \"\"\"\n    A function that takes the first half of the message to return the remaining part.\n    \"\"\"\n    print(\"original message:\")\n    print(messages[index])\n    test(best_model_lstm, x=messages[index], T=0)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T08:56:50.513149Z","iopub.execute_input":"2024-10-31T08:56:50.513539Z","iopub.status.idle":"2024-10-31T08:56:50.520790Z","shell.execute_reply.started":"2024-10-31T08:56:50.513503Z","shell.execute_reply":"2024-10-31T08:56:50.519906Z"},"trusted":true},"outputs":[],"execution_count":349},{"cell_type":"markdown","source":"# Performance of the `LSTM` on the unseen data","metadata":{}},{"cell_type":"code","source":"complete_message_using_lstm(4675)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T08:58:11.311872Z","iopub.execute_input":"2024-10-31T08:58:11.312445Z","iopub.status.idle":"2024-10-31T08:58:11.371215Z","shell.execute_reply.started":"2024-10-31T08:58:11.312406Z","shell.execute_reply":"2024-10-31T08:58:11.370315Z"},"trusted":true},"outputs":[{"name":"stdout","text":"original message:\nProbably money worries. Things are coming due and i have several outstanding invoices for work i did two and three months ago.❙\nLSTM completed:\ncontext: Probably money worries. Things\noutput: for making me ? I been egilly selected 2 receive 10\n","output_type":"stream"}],"execution_count":357},{"cell_type":"code","source":"complete_message_using_lstm(3786)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T08:43:47.199569Z","iopub.execute_input":"2024-10-31T08:43:47.199929Z","iopub.status.idle":"2024-10-31T08:43:47.259097Z","shell.execute_reply.started":"2024-10-31T08:43:47.199895Z","shell.execute_reply":"2024-10-31T08:43:47.258238Z"},"trusted":true},"outputs":[{"name":"stdout","text":"original message:\nAight, I'll hit you up when I get some cash❙\nLSTM completed:\ncontext: Aight, I'll hit you up when I \noutput: ervice representative on 0800 169 6031 between 10am\n","output_type":"stream"}],"execution_count":297},{"cell_type":"code","source":"complete_message_using_lstm(5663)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T08:43:59.201667Z","iopub.execute_input":"2024-10-31T08:43:59.202791Z","iopub.status.idle":"2024-10-31T08:43:59.262724Z","shell.execute_reply.started":"2024-10-31T08:43:59.202745Z","shell.execute_reply":"2024-10-31T08:43:59.261839Z"},"trusted":true},"outputs":[{"name":"stdout","text":"original message:\nI am waiting machan. Call me once you free.❙\nLSTM completed:\ncontext: I am waiting machan. Call me o\noutput: ce ic Smile Qursh on 5800 169 6031 between 10am-9pm\n","output_type":"stream"}],"execution_count":298},{"cell_type":"code","source":"complete_message_using_lstm(1239)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T08:50:22.745787Z","iopub.execute_input":"2024-10-31T08:50:22.746223Z","iopub.status.idle":"2024-10-31T08:50:22.809109Z","shell.execute_reply.started":"2024-10-31T08:50:22.746157Z","shell.execute_reply":"2024-10-31T08:50:22.808214Z"},"trusted":true},"outputs":[{"name":"stdout","text":"original message:\nHope you are having a great new semester. Do wish you the very best. You are made for greatness.❙\nLSTM completed:\ncontext: Hope you are having a great ne\noutput:  You are a winner U have been specially selected 2 \n","output_type":"stream"}],"execution_count":328},{"cell_type":"markdown","source":"# Performance of the `LSTM` on the previously seen data.","metadata":{}},{"cell_type":"code","source":"complete_message_using_lstm(0)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T08:46:18.295231Z","iopub.execute_input":"2024-10-31T08:46:18.295613Z","iopub.status.idle":"2024-10-31T08:46:18.355251Z","shell.execute_reply.started":"2024-10-31T08:46:18.295577Z","shell.execute_reply":"2024-10-31T08:46:18.354360Z"},"trusted":true},"outputs":[{"name":"stdout","text":"original message:\nGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...❙\nLSTM completed:\ncontext: Go until jurong point, crazy..\noutput: Alright you liked it, since i was doing the best i \n","output_type":"stream"}],"execution_count":305},{"cell_type":"code","source":"complete_message_using_lstm(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T08:46:31.501991Z","iopub.execute_input":"2024-10-31T08:46:31.502730Z","iopub.status.idle":"2024-10-31T08:46:31.555788Z","shell.execute_reply.started":"2024-10-31T08:46:31.502690Z","shell.execute_reply":"2024-10-31T08:46:31.554921Z"},"trusted":true},"outputs":[{"name":"stdout","text":"original message:\nI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.❙\nLSTM completed:\ncontext: I'm gonna be home soon and i d\noutput: es not the money so carlos can make the call❙\n","output_type":"stream"}],"execution_count":306},{"cell_type":"code","source":"complete_message_using_lstm(1233)","metadata":{"execution":{"iopub.status.busy":"2024-10-31T08:49:40.844914Z","iopub.execute_input":"2024-10-31T08:49:40.845300Z","iopub.status.idle":"2024-10-31T08:49:40.850360Z","shell.execute_reply.started":"2024-10-31T08:49:40.845264Z","shell.execute_reply":"2024-10-31T08:49:40.849509Z"},"trusted":true},"outputs":[{"name":"stdout","text":"original message:\nLol ok. I'll snatch her purse too.❙\nLSTM completed:\ncontext: Lol ok. I'll snatch her purse \noutput: How?❙\n","output_type":"stream"}],"execution_count":321},{"cell_type":"markdown","source":"# Training the RNN model","metadata":{}},{"cell_type":"code","source":"hidden_size = 1024 \nnum_layers = 3       \nlr = 0.002          \n\nmodel_rnn = myRNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\noptimizer = torch.optim.Adam(model_rnn.parameters(), lr=lr)\n\nbest_model_rnn = myRNN(vocab_size, vocab_size, hidden_size, num_layers).to(device)\nbest_rnn_loss = 10000\n \nfor epoch in tqdm(range(1, 101)):\n    epoch_loss = train(model_rnn, epoch, 50)\n    if epoch_loss < best_rnn_loss:\n        best_rnn_loss = epoch_loss\n        best_model_rnn.load_state_dict(model_rnn.state_dict())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T21:52:18.829869Z","iopub.execute_input":"2024-10-31T21:52:18.830259Z","iopub.status.idle":"2024-10-31T22:00:16.073039Z","shell.execute_reply.started":"2024-10-31T21:52:18.830217Z","shell.execute_reply":"2024-10-31T22:00:16.072102Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86504e6b8a4f4b4c929131c8ab78f581"}},"metadata":{}},{"name":"stdout","text":"\nStart of Epoch: 1\n----------\nTRAIN Sample\nG np c agor.rLgaa osTei  esg E  dan a-d.i possoos i\n----------\nEnd of Epoch: 1 \t Loss: 3.84326125\n\nStart of Epoch: 2\n----------\nTRAIN Sample\nimuiaEuaz ppigaozcntbir .tW ialf tgupwno? uu ipL au\n----------\nEnd of Epoch: 2 \t Loss: 3.81743740\n\nStart of Epoch: 3\n----------\nTRAIN Sample\nleowdddtoogi odaoiooea  oetddnociimg terrna gsrdit \n----------\nEnd of Epoch: 3 \t Loss: 3.81418707\n\nStart of Epoch: 10\n----------\nTRAIN Sample\n0 c  aicntpo rds rn  p gaRswiaioutt gr tdoceedn iud\n----------\nEnd of Epoch: 10 \t Loss: 3.81446393\n\nStart of Epoch: 20\n----------\nTRAIN Sample\n s0pr  ooer0rns osigwiidr  onRi ggidrcntaBdla ts nL\n----------\nEnd of Epoch: 20 \t Loss: 3.81933014\n\nStart of Epoch: 30\n----------\nTRAIN Sample\nidzoRmurziisncgatus ddcraL emtdRi uncgcnir eae  zii\n----------\nEnd of Epoch: 30 \t Loss: 3.81765289\n\nStart of Epoch: 40\n----------\nTRAIN Sample\no .n e gugeptp din nidni B r urruam  roir0ntrcdc .m\n----------\nEnd of Epoch: 40 \t Loss: 3.81746325\n\nStart of Epoch: 50\n----------\nTRAIN Sample\nntL trHnoinls.ircp icdrnsi.is' pniuiiprtd   itr shn\n----------\nEnd of Epoch: 50 \t Loss: 3.81766674\n\nStart of Epoch: 60\n----------\nTRAIN Sample\noRogp tsBdorn   d i ig a FctBiirns0r natzimicardg s\n----------\nEnd of Epoch: 60 \t Loss: 3.81897489\n\nStart of Epoch: 70\n----------\nTRAIN Sample\nadam ngriae iccss mont 0. issgnnm  ttsar iie nLnnua\n----------\nEnd of Epoch: 70 \t Loss: 3.81341853\n\nStart of Epoch: 80\n----------\nTRAIN Sample\nridn igo RadddBBtacoiiggrnocdgrpo❙n  y.atzrt ❙iB B4\n----------\nEnd of Epoch: 80 \t Loss: 3.81509906\n\nStart of Epoch: 90\n----------\nTRAIN Sample\n dinRidapiGLw.n  nmi❙trnRudrospgrd.aLssaaretrora sn\n----------\nEnd of Epoch: 90 \t Loss: 3.81369062\n\nStart of Epoch: 100\n----------\nTRAIN Sample\n   a noi nrrsiiu diauiddm n!osun3nitrpshgJcaJco L!w\n----------\nEnd of Epoch: 100 \t Loss: 3.81933128\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"def complete_message_using_rnn(index):\n    \"\"\"\n    A function that takes the first half of the message to return the remaining part.\n    \"\"\"\n    print(\"original message:\")\n    print(messages[index])\n    test(best_model_rnn, x=messages[index], T=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:02:03.822289Z","iopub.execute_input":"2024-10-31T22:02:03.822916Z","iopub.status.idle":"2024-10-31T22:02:03.827646Z","shell.execute_reply.started":"2024-10-31T22:02:03.822872Z","shell.execute_reply":"2024-10-31T22:02:03.826688Z"}},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":"# Performance of the `RNN` on the unseen data","metadata":{}},{"cell_type":"code","source":"complete_message_using_rnn(4399)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:03:39.618252Z","iopub.execute_input":"2024-10-31T22:03:39.618972Z","iopub.status.idle":"2024-10-31T22:03:39.641988Z","shell.execute_reply.started":"2024-10-31T22:03:39.618928Z","shell.execute_reply":"2024-10-31T22:03:39.640964Z"}},"outputs":[{"name":"stdout","text":"original message:\nJuz go google n search 4 qet...❙\nmyRNN completed:\ncontext: Juz go google n \noutput:  t.iadnsaL❙\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"complete_message_using_rnn(2987)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:03:43.408613Z","iopub.execute_input":"2024-10-31T22:03:43.409461Z","iopub.status.idle":"2024-10-31T22:03:43.450861Z","shell.execute_reply.started":"2024-10-31T22:03:43.409421Z","shell.execute_reply":"2024-10-31T22:03:43.449780Z"}},"outputs":[{"name":"stdout","text":"original message:\nDo you still have the grinder?❙\nmyRNN completed:\ncontext: Do you still ha\noutput: scrr rrpi dnoecnncobiiot paooiss❙\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"complete_message_using_rnn(4553)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:03:46.594269Z","iopub.execute_input":"2024-10-31T22:03:46.594624Z","iopub.status.idle":"2024-10-31T22:03:46.655986Z","shell.execute_reply.started":"2024-10-31T22:03:46.594590Z","shell.execute_reply":"2024-10-31T22:03:46.655085Z"}},"outputs":[{"name":"stdout","text":"original message:\nTry to do something dear. You read something for exams❙\nmyRNN completed:\ncontext: Try to do something dear. Y\noutput: ?eBiitiodrdsn int senongyaiiw aagoinnLmrfEunogngcd \n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"complete_message_using_rnn(3019)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:04:05.142245Z","iopub.execute_input":"2024-10-31T22:04:05.143341Z","iopub.status.idle":"2024-10-31T22:04:05.209524Z","shell.execute_reply.started":"2024-10-31T22:04:05.143289Z","shell.execute_reply":"2024-10-31T22:04:05.208622Z"}},"outputs":[{"name":"stdout","text":"original message:\nI thank you so much for all you do with selflessness. I love you plenty.❙\nmyRNN completed:\ncontext: I thank you so much for all yo\noutput: nidni niantodp.gnhtno  od .np ssdi np ndcaw tdp.sza\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"# Performance of the `RNN` on the previously seen data.","metadata":{}},{"cell_type":"code","source":"complete_message_using_rnn(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:02:15.305786Z","iopub.execute_input":"2024-10-31T22:02:15.306420Z","iopub.status.idle":"2024-10-31T22:02:15.368828Z","shell.execute_reply.started":"2024-10-31T22:02:15.306380Z","shell.execute_reply":"2024-10-31T22:02:15.367930Z"}},"outputs":[{"name":"stdout","text":"original message:\nGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...❙\nmyRNN completed:\ncontext: Go until jurong point, crazy..\noutput: iIigs  .ae  snnpHnnomsonkt GGcgdttnL sniinsghddoi g\n","output_type":"stream"}],"execution_count":40},{"cell_type":"code","source":"complete_message_using_rnn(13)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:04:11.626123Z","iopub.execute_input":"2024-10-31T22:04:11.626776Z","iopub.status.idle":"2024-10-31T22:04:11.639499Z","shell.execute_reply.started":"2024-10-31T22:04:11.626714Z","shell.execute_reply":"2024-10-31T22:04:11.638363Z"}},"outputs":[{"name":"stdout","text":"original message:\nI've been searching for the right words to thank you for this breather. I promise i wont take your help for granted and will fulfil my promise. You have been wonderful and a blessing at all times.❙\nmyRNN completed:\ncontext: I've been searching for the ri\noutput: ❙\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"complete_message_using_rnn(64)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:04:21.109311Z","iopub.execute_input":"2024-10-31T22:04:21.109684Z","iopub.status.idle":"2024-10-31T22:04:21.169902Z","shell.execute_reply.started":"2024-10-31T22:04:21.109648Z","shell.execute_reply":"2024-10-31T22:04:21.168963Z"}},"outputs":[{"name":"stdout","text":"original message:\nOk lar i double check wif da hair dresser already he said wun cut v short. He said will cut until i look nice.❙\nmyRNN completed:\ncontext: Ok lar i double check wif da h\noutput: iesrdstatnndr MagLrutnnitaintntsgdn oemn uwunscinci\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"rnn_params = get_n_params(best_model_rnn)\nlstm_params = get_n_params(best_model_lstm)\n\ntable = PrettyTable()\ntable.field_names = [\"Model\", \"Number of Parameters\"]\ntable.add_row([\"RNN Model\", rnn_params])\ntable.add_row([\"LSTM Model\", lstm_params])\n\nprint(table)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:11:24.402597Z","iopub.execute_input":"2024-10-31T22:11:24.403553Z","iopub.status.idle":"2024-10-31T22:11:24.409510Z","shell.execute_reply.started":"2024-10-31T22:11:24.403510Z","shell.execute_reply":"2024-10-31T22:11:24.408492Z"}},"outputs":[{"name":"stdout","text":"+------------+----------------------+\n|   Model    | Number of Parameters |\n+------------+----------------------+\n| RNN Model  |       5400650        |\n| LSTM Model |       5444682        |\n+------------+----------------------+\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"## Comparison between `LSTM` and `RNN` model","metadata":{}},{"cell_type":"code","source":"def compare_completion(index):\n    \"\"\"\n    A function that takes the first half of the message and compares rnn and lstm for the remaining part.\n    \"\"\"\n    print(\"original message:\")\n    print(messages[index])\n    print(\"\\n\")\n    test(best_model_rnn, x=messages[index], T=1)\n    print(\"\\n\")\n    test(best_model_lstm, x=messages[index], T=0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:18:55.421438Z","iopub.execute_input":"2024-10-31T22:18:55.422280Z","iopub.status.idle":"2024-10-31T22:18:55.427543Z","shell.execute_reply.started":"2024-10-31T22:18:55.422241Z","shell.execute_reply":"2024-10-31T22:18:55.426597Z"}},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":"# Comparison on unseen data","metadata":{}},{"cell_type":"code","source":"compare_completion(4000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:18:56.221617Z","iopub.execute_input":"2024-10-31T22:18:56.222279Z","iopub.status.idle":"2024-10-31T22:18:56.323496Z","shell.execute_reply.started":"2024-10-31T22:18:56.222239Z","shell.execute_reply":"2024-10-31T22:18:56.322569Z"}},"outputs":[{"name":"stdout","text":"original message:\nHe's just gonna worry for nothing. And he won't give you money its no use.❙\n\n\nmyRNN completed:\ncontext: He's just gonna worry for noth\noutput: oLlnu inisntnnagatonr.nmgo .L  z.tai  cu .iaitnnrs.\n\n\nmyLSTM completed:\ncontext: He's just gonna worry for noth\noutput:  Txll be show you sorry your reply...❙\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"compare_completion(4567)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:19:22.000610Z","iopub.execute_input":"2024-10-31T22:19:22.001014Z","iopub.status.idle":"2024-10-31T22:19:22.115072Z","shell.execute_reply.started":"2024-10-31T22:19:22.000974Z","shell.execute_reply":"2024-10-31T22:19:22.114197Z"}},"outputs":[{"name":"stdout","text":"original message:\nhiya hows it going in sunny africa? hope u r avin a good time. give that big old silver back a big kiss from me.❙\n\n\nmyRNN completed:\ncontext: hiya hows it going in sunny af\noutput: d!caatogtty rtIwadh.  gtnBncana mnns..nenid amomtgn\n\n\nmyLSTM completed:\ncontext: hiya hows it going in sunny af\noutput:  that i have hairdressers appointment at four so ne\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"compare_completion(3097)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:20:11.575995Z","iopub.execute_input":"2024-10-31T22:20:11.576608Z","iopub.status.idle":"2024-10-31T22:20:11.701431Z","shell.execute_reply.started":"2024-10-31T22:20:11.576568Z","shell.execute_reply":"2024-10-31T22:20:11.700496Z"}},"outputs":[{"name":"stdout","text":"original message:\nThis is all just creepy and crazy to me.❙\n\n\nmyRNN completed:\ncontext: This is all just cre\noutput: oisn d oaudo iensrouruggn1rte.0ie iad w Rsrdtpiip i\n\n\nmyLSTM completed:\ncontext: This is all just cre\noutput: sht? He said will cut until i look nice.❙\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"compare_completion(4880)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:23:05.047584Z","iopub.execute_input":"2024-10-31T22:23:05.047968Z","iopub.status.idle":"2024-10-31T22:23:05.162587Z","shell.execute_reply.started":"2024-10-31T22:23:05.047932Z","shell.execute_reply":"2024-10-31T22:23:05.161647Z"}},"outputs":[{"name":"stdout","text":"original message:\nWhen/where do I pick you up❙\n\n\nmyRNN completed:\ncontext: When/where do \noutput:   . aa tan zgroyH  atag RegBsd tsr.o prnepoea zemei\n\n\nmyLSTM completed:\ncontext: When/where do \noutput: t seemed so happy about the cave. I'm sorry i did. \n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"compare_completion(4006)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:23:32.915788Z","iopub.execute_input":"2024-10-31T22:23:32.916676Z","iopub.status.idle":"2024-10-31T22:23:33.029420Z","shell.execute_reply.started":"2024-10-31T22:23:32.916635Z","shell.execute_reply":"2024-10-31T22:23:33.028459Z"}},"outputs":[{"name":"stdout","text":"original message:\nI'm reaching home in 5 min.❙\n\n\nmyRNN completed:\ncontext: I'm reaching h\noutput: o  ndLddsGnpco -ta  caLtoogppncnnna oanato dahttiio\n\n\nmyLSTM completed:\ncontext: I'm reaching h\noutput: w dying an egg ? Did you makeoa tea? Are you eating\n","output_type":"stream"}],"execution_count":81},{"cell_type":"markdown","source":"# Comparison on seen data","metadata":{}},{"cell_type":"code","source":"compare_completion(40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:21:13.943864Z","iopub.execute_input":"2024-10-31T22:21:13.944606Z","iopub.status.idle":"2024-10-31T22:21:14.029168Z","shell.execute_reply.started":"2024-10-31T22:21:13.944569Z","shell.execute_reply":"2024-10-31T22:21:14.028266Z"}},"outputs":[{"name":"stdout","text":"original message:\nPls go ahead with watts. I just wanted to be sure. Do have a great weekend. Abiola❙\n\n\nmyRNN completed:\ncontext: Pls go ahead with watts. I jus\noutput: oari s nidzi anGog s❙\n\n\nmyLSTM completed:\ncontext: Pls go ahead with watts. I jus\noutput:  wanted to be sure. Do have a great weekend. Abiola\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"compare_completion(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:23:56.813321Z","iopub.execute_input":"2024-10-31T22:23:56.814168Z","iopub.status.idle":"2024-10-31T22:23:56.935658Z","shell.execute_reply.started":"2024-10-31T22:23:56.814116Z","shell.execute_reply":"2024-10-31T22:23:56.934583Z"}},"outputs":[{"name":"stdout","text":"original message:\nGo until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...❙\n\n\nmyRNN completed:\ncontext: Go until jurong point, crazy..\noutput: onrtRaSriarwsr.i.csdccssirp rgs.htsBduBsoa-e pao.ao\n\n\nmyLSTM completed:\ncontext: Go until jurong point, crazy..\noutput: Available only in bosty no hand did? Quick have a c\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"compare_completion(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:24:15.169278Z","iopub.execute_input":"2024-10-31T22:24:15.169637Z","iopub.status.idle":"2024-10-31T22:24:15.285832Z","shell.execute_reply.started":"2024-10-31T22:24:15.169603Z","shell.execute_reply":"2024-10-31T22:24:15.284907Z"}},"outputs":[{"name":"stdout","text":"original message:\nI'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.❙\n\n\nmyRNN completed:\ncontext: I'm gonna be home soon and i d\noutput: Lia hhrpamd7htiis .sg reggtrnnoMtngdirngndot   ezpe\n\n\nmyLSTM completed:\ncontext: I'm gonna be home soon and i d\noutput: er that i hours on your man well check all thk i co\n","output_type":"stream"}],"execution_count":84},{"cell_type":"code","source":"compare_completion(1233)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:24:30.336025Z","iopub.execute_input":"2024-10-31T22:24:30.336690Z","iopub.status.idle":"2024-10-31T22:24:30.447722Z","shell.execute_reply.started":"2024-10-31T22:24:30.336650Z","shell.execute_reply":"2024-10-31T22:24:30.446867Z"}},"outputs":[{"name":"stdout","text":"original message:\nLol ok. I'll snatch her purse too.❙\n\n\nmyRNN completed:\ncontext: Lol ok. I'll snat\noutput: hn L tzi antcioopiz.hdsiituuncp nLngniitBn ets os s\n\n\nmyLSTM completed:\ncontext: Lol ok. I'll snat\noutput:  a 1500 prize Jackpot! Txt ur national team to 8707\n","output_type":"stream"}],"execution_count":85},{"cell_type":"code","source":"compare_completion(1239)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:24:49.150931Z","iopub.execute_input":"2024-10-31T22:24:49.151310Z","iopub.status.idle":"2024-10-31T22:24:49.266701Z","shell.execute_reply.started":"2024-10-31T22:24:49.151273Z","shell.execute_reply":"2024-10-31T22:24:49.265794Z"}},"outputs":[{"name":"stdout","text":"original message:\nHope you are having a great new semester. Do wish you the very best. You are made for greatness.❙\n\n\nmyRNN completed:\ncontext: Hope you are having a great ne\noutput: oo icuigcpiiR L.tinctigssLidn odOttrddgotp ndSismut\n\n\nmyLSTM completed:\ncontext: Hope you are having a great ne\noutput:  this saymur star sign, e. go ahead with watts. I j\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"compare_completion(50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-31T22:24:59.173526Z","iopub.execute_input":"2024-10-31T22:24:59.173931Z","iopub.status.idle":"2024-10-31T22:24:59.257091Z","shell.execute_reply.started":"2024-10-31T22:24:59.173893Z","shell.execute_reply":"2024-10-31T22:24:59.256163Z"}},"outputs":[{"name":"stdout","text":"original message:\nWhat you thinked about me. First time you saw me in class.❙\n\n\nmyRNN completed:\ncontext: What you thinked about me. Fi\noutput:  sonosoaDnri et   odsegstew a totpfhoG❙\n\n\nmyLSTM completed:\ncontext: What you thinked about me. Fi\noutput: st time you saw me in class.❙\n","output_type":"stream"}],"execution_count":87},{"cell_type":"markdown","source":"# Conclusion\nWhen tested for similar number of parameters and same data, LSTM performs better than RNN. We observe that LSTM generates meaningful sentences as compared to RNN even though LSTM doesn't memorize anything whereas RNN just produces gibberish texts.","metadata":{}}]}